{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 0\n",
    "\n",
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"images/MLU_logo.png\"></div>\n",
    "\n",
    "# MLU Operationalizing Generative AI with LLMOps \n",
    "# <a name=\"p0\">Lab 1: Interacting with the LLM-powered application</a>\n",
    "\n",
    "In this lab you will familiarize yourself with the [GenAI with Bedrock](https://create.hub.amazon.dev/source-applications/genai-python-lambda) service that you have created via [BuilderHub Create](https://create.hub.amazon.dev). You will send requests with various questions to the API Gateway / AWS Lambda service and get answers back to gain an initial understanding of what the service is capable of. \n",
    "\n",
    "Next, you will experiment with Amazon Bedrock directly from this notebook, which you can use as your local playground. This will allow you to test different models and inference settings available in Bedrock, try out prompts and prompting techniques, and obtain responses from the system. In a real development scenario, **you can use notebooks for rapid and easy iterations** prior to deploying changes to the main package. \n",
    "\n",
    "Finally, you will implement changes to the main package and deploy them via the CDK CLI of your CDK package. This will update the code in the AWS Lambda function, allowing you to modify the behavior of the live system. Once your changes have been deployed, you will be able to check that your modifications have indeed updated the LLM-powered service. \n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Familiarize yourself with the deployed service](#1)\n",
    "2. [Explore LLM capabilities via prompting in Amazon Bedrock](#2)\n",
    "3. [Implement changes to the system and deploy with CDK](#3)\n",
    "\n",
    "<br/>\n",
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        This notebook assumes that you have already completed the following tasks, as described in the Lab Setup Instructions document:<br/>\n",
    "        <ul>\n",
    "            <li>Created a <a style=\"color: lightskyblue;\" href=\"https://create.hub.amazon.dev/source-applications/genai-python-lambda\">GenAI with Bedrock</a> application with <code style=\"color: lightcoral;\">{Alias}MLUCourseLLMOps</code> as Clone Name.</li><br/>\n",
    "            <li>Enabled model access to Claude 3 Haiku and Sonnet, and Mistral 7B Instruct on Amazon Bedrock in your AWS account.</li><br/>\n",
    "            <li>Created a workspace on your development environment and pulled all packages included in the application.</li><br/>\n",
    "            <li>Built your Experiment package in your development environment with <code style=\"color: lightcoral;\">brazil-build release</code>.</li>\n",
    "        </ul>\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Jupyter Kernel\n",
    "\n",
    "In order to run this notebook, you need to use a Kernel. We will use the Kernel from the Python virtual environment provided with this package. \n",
    "\n",
    "**In VSCode**\n",
    "  - Click on \"Select Kernel\" on the top right of this window.\n",
    "  - Click on \"Python Environments\" on the text input bar at the top of this window.\n",
    "  - Select the `.venv (Python 3.12.x)` Virtual env, located in `{WorkspaceRoot}/src/{Alias}MLUCourseLLMOpsExperiment/.venv/bin/python`\n",
    "  - Double check that the Kernel shown on the top right of this window reads `.venv (Python 3.12.x)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about your development environment\n",
    "\n",
    "One of the key goals of this lab is to give you an experience as similar as possible to a real LLM operationalization process. \n",
    "\n",
    "Besides the data science environment based on Jupyter notebooks, where you can experiment with LLM models, you will also be required to run commands in a terminal window.\n",
    "\n",
    "Every time you see  an instruction to open a terminal and run a command, feel free to choose the terminal you are more comfortable with: \n",
    "+ A Terminal window with SSH to your Cloud Desktop, or\n",
    "+ The VS Code Terminal window. In this case, make sure to open the terminal in the same window that is connected to your Cloud Destop environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "###### 1\n",
    "## <a>Part 1 - Familiarize yourself with the deployed service</a>\n",
    "([Go to top](#0))\n",
    "\n",
    "Once your pipeline has deployed successfully to the alpha stage, you have an API endpoint that can be used to send prompts to a Large Language Model (LLM) hosted on Amazon Bedrock. This application uses [Retrieval Augmented Generation (RAG)](https://aws.amazon.com/what-is/retrieval-augmented-generation/), a technique that pulls from a vector store to retrieve relevant documents. The retrieved information is inserted into the context that's sent to the model via the prompt, which enhances the quality of the model's response. \n",
    "\n",
    "During application creation, we have stored documentation about AWS Lambda in Amazon Kendra, which turns the initial deployed service into an expert on questions related to AWS Lambda.\n",
    "\n",
    "The diagram below clarifies the components of the application's architecture:\n",
    "\n",
    "![System_Architecture Image depicting the steps 1-8 described in the text below.](images/BasicGenAI-Tutorial.drawio.png)\n",
    "\n",
    "1. Client makes an **API request to an API Gateway endpoint**. The request is signed with SigV4. \n",
    "2. API Gateway receives the request and invokes the configured **AWS Lambda function** with the request payload.\n",
    "3. Lambda function handler invokes **Amazon Kendra API** with the input query. \n",
    "4. Amazon Kendra returns the list of **relevant document chunks**.\n",
    "5. Lambda function handler builds the **prompt with the input query and the context** (i.e. relevant document chunks) and invokes Bedrock API. \n",
    "6. Bedrock API performs the **model inference** on the prompt and returns the results to the function handler.\n",
    "7. The function builds the **response** structure and returns to API Gateway. \n",
    "8. API Gateway returns the **API response to the client**.\n",
    "\n",
    "Independently from this workflow, Amazon Kendra asynchronously imports the knowledge base from a designated S3 location to its index. \n",
    "\n",
    "### Getting credentials to connect to the AWS account\n",
    "\n",
    "Let's test how the deployed application responds to your questions by sending requests to the API running in your alpha AWS account. The API is protected by IAM authentication, which means that we require a credential for an IAM role to sign the request. \n",
    "\n",
    "You will use the `MLU-LLMOps-Burner` profile that you have created with [Ada (Authorization and Auditing for Networking)](https://w.amazon.com/bin/view/Public/Ada) during the Lab Setup process. \n",
    "\n",
    "Take a look at your `~/.config/ada/profile.json` that must contain a profile named `MLU-LLMOps-Burner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Profiles\": [\n",
      "    {\n",
      "      \"Provider\": \"conduit\",\n",
      "      \"Account\": \"654654505854\",\n",
      "      \"Role\": \"IibsAdminAccess-DO-NOT-DELETE\",\n",
      "      \"Partition\": \"aws\",\n",
      "      \"Profile\": \"aqd\"\n",
      "    },\n",
      "    {\n",
      "      \"Provider\": \"conduit\",\n",
      "      \"Account\": \"961341554577\",\n",
      "      \"Region\": \"us-west-2\",\n",
      "      \"Role\": \"IibsAdminAccess-DO-NOT-DELETE\",\n",
      "      \"Partition\": \"aws\",\n",
      "      \"Profile\": \"MLU-LLMOps-Burner\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat $HOME/.config/ada/profile.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output above does not contain an entry for a `conduit` profile named `MLU-LLMOps-Burner`, please double check that you have properly created a `MLU-LLMOps-Burner` profile with `ada profile add`. \n",
    "\n",
    "Go back to the Lab Setup instructions, visit the `Bootstrap the AWS Burner Account` section and review the steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that the created profile can correctly connect to your AWS Burner account. \n",
    "\n",
    "**Run the cell below. If you receive a response containing `UserId`, `Account` and `Arn` corresponding to your AWS Burner account, your AWS account access has been properly set up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"UserId\": \"AROA57VDL76IRL6OEPC4F:koachang@MIDWAY.AMAZON.COM\",\n",
      "    \"Account\": \"961341554577\",\n",
      "    \"Arn\": \"arn:aws:sts::961341554577:assumed-role/IibsAdminAccess-DO-NOT-DELETE/koachang@MIDWAY.AMAZON.COM\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws --profile=MLU-LLMOps-Burner sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boto3 version**\n",
    "\n",
    "Before we proceed with the rest of the notebook, let's check the `boto3` version that's available through your virtual environment. The GenAI application runs with `\"boto3 >=1.34.143\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.34.153'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import boto3 and check version\n",
    "import boto3 \n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        If the <code style=\"color: lightcoral;\">boto3</code> version displayed above is equal or higher than <code style=\"color: lightcoral;\">1.34.143</code>, you can ignore this box. If it's lower than <code style=\"color: lightcoral;\">1.34.143</code>, you should follow the steps below to update it:<br/>\n",
    "        <ul>\n",
    "            <li>Open file <code style=\"color: lightcoral;\">pyproject.toml</code> located inside your Experiment package.</li><br/>\n",
    "            <li>Edit section <code style=\"color: lightcoral;\">[project.optional-dependencies]</code> and replace the <code style=\"color: lightcoral;\">boto3</code> line with <code style=\"color: lightcoral;\">\"boto3 >=1.34.143\"</code>.</li><br/>\n",
    "            <li>Delete file <code style=\"color: lightcoral;\">requirements.txt</code> from the root folder of the Experiment package by running <code style=\"color: lightcoral;\">rm requirements.txt</code>. Don't worry, this file will be regenerated later.</li><br/>\n",
    "            <li>From a Terminal in your Cloud Desktop, navigate to your Experiment package and activate the virtual environment by running <code style=\"color: lightcoral;\">source .venv/bin/activate</code>.</li><br/>\n",
    "            <li>From that same Terminal in your Cloud Desktop, install <code style=\"color: lightcoral;\">pip-tools</code> by running <code style=\"color: lightcoral;\">python -m pip install pip-tools</code>. Then regenerate the lock file <code style=\"color: lightcoral;\">requirements.txt</code> by running <code style=\"color: lightcoral;\">pip-compile --extra=dev</code>. This step might take a few minutes to complete.</li><br/>\n",
    "            <li>Build the Experiment package again by running<code style=\"color: lightcoral;\">brazil-build release</code>.</li><br/>\n",
    "            <li>Restart the Kernel of this notebook and re-run all cells up to this point. The <code style=\"color: lightcoral;\">boto3</code> version should now be <code style=\"color: lightcoral;\">>=1.34.143</code>. You can then continue running the cells below.</li>\n",
    "        </ul>\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the URL of your deployed service\n",
    "\n",
    "To make a request to the deployed GenAI application, you first need to retrieve the live API endpoint URL. We will use [AWS Signature Version 4](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) to authenticate the request. Python package [requests-auth-aws-sigv4](https://pypi.org/project/requests-auth-aws-sigv4/) provides an authentication class to add AWS Signature Version 4 authentication information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import the library that support AWS SigV4 requests\n",
    "!pip3 install -q requests-auth-aws-sigv4\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from requests_auth_aws_sigv4 import AWSSigV4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the API endpoing URL, you need to look into the CloudFormation Stacks that BuilderHub Create has created when it cloned the GenAI application. Take a quick look at your [CloudFormation stacks](https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks?filteringText=&filteringStatus=active&viewNested=true) in your AWS account, and you will find several entries named after your cloned package. \n",
    "\n",
    "In the next cell you set variable `MAIN_PACKAGE_NAME` to the name of your cloned GenAI application, using the naming convention for this course. Then you make a request to the CloudFormation service to retrieve information about the stack that contains the API endpoint. That is the important piece of information that you will need to make requests in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://denx1se8e0.execute-api.us-west-2.amazonaws.com/live/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start boto3 session using the credentials from your AWS Burner account\n",
    "session = boto3.Session(profile_name=\"MLU-LLMOps-Burner\")\n",
    "\n",
    "# Retrieve your alias from your $USER variable and assemble the name of your cloned application\n",
    "# Notice this will only work if you have adhered to the previously-specified naming convention for the cloned application\n",
    "alias = %env USER\n",
    "MAIN_PACKAGE_NAME = f\"{alias.capitalize()}MLUCourseLLMOps\"\n",
    "\n",
    "# Set up authentication\n",
    "aws_auth = AWSSigV4(\"cloudformation\", region=\"us-west-2\", session=session)\n",
    "\n",
    "# URL to request information about a particular stack in CloudFormation\n",
    "url = f\"https://cloudformation.us-west-2.amazonaws.com?Action=DescribeStacks&StackName={MAIN_PACKAGE_NAME}-Service-alpha\"\n",
    "\n",
    "# Request response in JSON format\n",
    "headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# Send request to CloudFormation\n",
    "r = requests.request(\"GET\", url, auth=aws_auth, headers=headers)\n",
    "\n",
    "# Locate the relevant key in the response that contains the API URL\n",
    "outputs = r.json()[\"DescribeStacksResponse\"][\"DescribeStacksResult\"][\"Stacks\"][0][\"Outputs\"]\n",
    "api_endpoint = [output for output in outputs if output[\"ExportName\"]==f\"{MAIN_PACKAGE_NAME}-ApiUrl\"][0][\"OutputValue\"]\n",
    "display(api_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; margin: 15px;\" src=\"images/MLU_question.png\" alt=\"MLU question\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Troubleshooting</b><p/>\n",
    "        <p>If the cell above throws an error, double check that your AWS credentials are valid. Remember that you're accessing AWS service via your AWS Burner account and the credentials are managed by profile <code>MLU-LLMOps-Burner</code> that was created with <code>ada</code>.</p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        The same information about the API URL can be retrieved from the command line with <code style=\"color: lightcoral;\">aws</code> in a Cloud Desktop terminal window.\n",
    "    <br/><br/>\n",
    "        Try from the Terminal in your Cloud Desktop! \n",
    "    <br/><br/>\n",
    "        Execute the cell below to get the exact commands that you can run in the command line to access the value of your API endpoint URL.\n",
    "    </span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Copy this command and run it on your Cloud Desktop terminal to output your API endpoint"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws cloudformation describe-stacks \\\n",
      " --profile MLU-LLMOps-Burner \\\n",
      " --stack-name KoachangMLUCourseLLMOps-Service-alpha \\\n",
      " --query 'Stacks[0].Outputs[?ExportName==`KoachangMLUCourseLLMOps-ApiUrl`].OutputValue' \\\n",
      " --output text | cat \n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"### Copy this command and run it on your Cloud Desktop terminal to output your API endpoint\"))\n",
    "\n",
    "print(f\"aws cloudformation describe-stacks \\\\\\n --profile MLU-LLMOps-Burner \\\\\\n --stack-name {MAIN_PACKAGE_NAME}-Service-alpha \\\\\\n --query 'Stacks[0].Outputs[?ExportName==`{MAIN_PACKAGE_NAME}-ApiUrl`].OutputValue' \\\\\\n --output text | cat \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a first request to the deployed API\n",
    "\n",
    "You can now interact with your GenAI service and send questions to the LLM using Python library `awscurl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library to make API requests to the deployed service\n",
    "from awscurl.awscurl import make_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a first request to the system, along with a question about AWS Lambda, and observe how the LLM is able to return a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is the maximum size of the ephemeral storage allowed by AWS Lambda?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "According to the context provided, the maximum size of the ephemeral storage (the /tmp directory) allowed by AWS Lambda is 10,240 MB (10 GB). The context states that you can configure the size of a function's /tmp directory in the Lambda console, and set a whole number value between 512 MB and 10,240 MB, in 1-MB increments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the maximum size of the ephemeral storage allowed by AWS Lambda?\"\n",
    "\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "response = make_request(\n",
    "    uri=api_endpoint,\n",
    "    headers=headers,\n",
    "    method=\"POST\",\n",
    "    service=\"execute-api\",\n",
    "    data=json.dumps(\n",
    "        {\"question\": question}\n",
    "    ),\n",
    "    region=\"us-west-2\",\n",
    "    access_key=credentials.access_key,\n",
    "    secret_key=credentials.secret_key,\n",
    "    security_token=credentials.token,\n",
    "    data_binary=False,\n",
    ")\n",
    "\n",
    "display(Markdown(f\"**{question}**\"))\n",
    "Markdown(response.json().get(\"answer\", response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the LLM response with the actual answer from the [AWS Lambda documentation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-ephemeralstorage.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        The same response can be retrieved from the command line in the Terminal with <code style=\"color: lightcoral;\">awscurl</code> in a Cloud Desktop terminal window.\n",
    "    <br/><br/>\n",
    "    Try from the Terminal in your Cloud Desktop! \n",
    "    <br/><br/>\n",
    "    You can execute the next cell to get the exact command that you can run in the command line to send a request to the live system. \n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Copy these commands and run them on your Cloud Desktop terminal to send a question to the system"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws_json=$(ada credentials print --profile MLU-LLMOps-Burner) export AWS_ACCESS_KEY_ID=$(echo $aws_json | jq -r .AccessKeyId) AWS_SECRET_ACCESS_KEY=$(echo $aws_json | jq -r .SecretAccessKey) AWS_SESSION_TOKEN=$(echo $aws_json | jq -r .SessionToken) \n",
      "awscurl https://denx1se8e0.execute-api.us-west-2.amazonaws.com/live/ \\\n",
      " --region us-west-2 \\\n",
      " --service execute-api \\\n",
      " -X POST \\\n",
      " --access_key $AWS_ACCESS_KEY_ID \\\n",
      " --secret_key $AWS_SECRET_ACCESS_KEY \\\n",
      " --security_token $AWS_SESSION_TOKEN \\\n",
      " -d '{\"question\": \"What is the maximum size of the ephemeral storage allowed by AWS Lambda?\"}' \n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### Copy these commands and run them on your Cloud Desktop terminal to send a question to the system\"))\n",
    "\n",
    "print(\"aws_json=$(ada credentials print --profile MLU-LLMOps-Burner) export AWS_ACCESS_KEY_ID=$(echo $aws_json | jq -r .AccessKeyId) AWS_SECRET_ACCESS_KEY=$(echo $aws_json | jq -r .SecretAccessKey) AWS_SESSION_TOKEN=$(echo $aws_json | jq -r .SessionToken) \")\n",
    "\n",
    "question_json = json.dumps({\"question\": \"What is the maximum size of the ephemeral storage allowed by AWS Lambda?\"})\n",
    "\n",
    "print(f\"awscurl {api_endpoint} \\\\\\n --region us-west-2 \\\\\\n --service execute-api \\\\\\n -X POST \\\\\\n --access_key $AWS_ACCESS_KEY_ID \\\\\\n --secret_key $AWS_SECRET_ACCESS_KEY \\\\\\n --security_token $AWS_SESSION_TOKEN \\\\\\n -d '{question_json}' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function to call the live GenAI service with any question. This function will return a generic error string if the request comes back from Lambda with an error message, i.e. a status code other than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_live_request(question):\n",
    "    credentials = session.get_credentials() \n",
    "    response = make_request(\n",
    "        uri=api_endpoint,\n",
    "        headers=headers,\n",
    "        method=\"POST\",\n",
    "        service=\"execute-api\",\n",
    "        data=json.dumps({\"question\": question}),\n",
    "        region=\"us-west-2\",\n",
    "        access_key=credentials.access_key,\n",
    "        secret_key=credentials.secret_key,\n",
    "        security_token=credentials.token,\n",
    "        data_binary=False,\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        return \"Request failed. Check the CloudWatch logs in your Lambda application.\"\n",
    "    else:\n",
    "        return response.json().get(\"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_challenge.png\" alt=\"MLU challenge\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b><p/>\n",
    "        <p><b>Exercise 1.</b> It is now your turn to <b>interact with the deployed system</b>:</p>\n",
    "            <ol>\n",
    "                <li>Try asking other questions about <b>AWS Lambda</b>. Are the answers correct, to the best of your knowledge?</li>\n",
    "                <li>Ask questions about <b>other AWS systems</b>. How good are the provided responses?</li>\n",
    "                <li>Inquire about general <b>topics unrelated to AWS</b>. Is the system able to generate correct answers?</li>\n",
    "            </ol>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know when Amazon Bedrock was created. The information provided does not contain any details about Amazon Bedrock.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "make_live_request(\"When was Amazon Bedrock created?\")\n",
    "\n",
    "\n",
    "############# END OF CODE ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Request failed. Check the CloudWatch logs in your Lambda application.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_live_request(\"When was Elon Musk born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        If you get <code style=\"color: lightcoral;\">Request failed</code> responses, your deployed AWS Lambda function has encountered an error. \n",
    "    <br/><br/>\n",
    "        You can inspect the errors from your live invocations to AWS Lambda in CloudWatch.<br/>\n",
    "        Run the cell below to get a direct link to CloudWatch Log Groups for your deployed application.<br/>\n",
    "        You need to be logged into your Burner account to access them.\n",
    "    <br/><br/>Locate the latest log stream and inspect the error messages. Can you troubleshoot their cause?\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Go to the following link to access CloudWatch and inspect potential errors from your application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logsV2:log-groups$3FlogGroupNameFilter$3DKoachangMLUCourseLLMOps-Service-alpha-KoachangMLUCourseLLMOps'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Go to the following link to access CloudWatch and inspect potential errors from your application.\"))\n",
    "\n",
    "display(\n",
    "    f\"https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logsV2:log-groups$3FlogGroupNameFilter$3D{MAIN_PACKAGE_NAME}-Service-alpha-{MAIN_PACKAGE_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_question.png\" alt=\"MLU solution\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b><p/>\n",
    "        <p><b>Exercise 1.</b> Below we provide you with example questions to interact with the deployed service.</p>\n",
    "        <p>Remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display the sample solutions.</p>\n",
    "        <p>You can then re-run the cell to see its output.</p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Questions about AWS Lambda"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**How is AWS Lambda different from EC2?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The key differences between AWS Lambda and Amazon EC2 are:\n",
       "\n",
       "1. Compute management: With AWS Lambda, you don't need to manage any compute infrastructure. Lambda handles the provisioning, scaling, and management of the compute resources. In contrast, with Amazon EC2, you are responsible for provisioning, managing, and scaling the EC2 instances yourself.\n",
       "\n",
       "2. Pricing model: Lambda charges based on the number of requests and the compute time required to execute your code, whereas EC2 charges based on the duration you run the instances and the instance type.\n",
       "\n",
       "3. Scalability: Lambda automatically scales up or down based on the incoming workload, while with EC2 you need to manually scale the instances to handle increased demand.\n",
       "\n",
       "4. Programming model: Lambda functions are event-driven and execute in response to specific events or triggers, while EC2 instances run continuously and you need to manage the application lifecycle.\n",
       "\n",
       "5. Customization: With EC2, you have full control over the operating system, software stack, and configurations, whereas with Lambda, you are limited to the provided runtimes and cannot customize the underlying operating system.\n",
       "\n",
       "In summary, Lambda is a serverless compute service that abstracts away the infrastructure management, while EC2 provides more control over the underlying compute resources but requires more operational overhead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What happens to an AWS Lambda function if a Lambda layer is deleted?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "If a Lambda layer that is being used by an AWS Lambda function is deleted, the following happens:\n",
       "\n",
       "- The Lambda function can continue to use the deleted layer version. The function will continue to run as if the layer version still exists.\n",
       "- However, you cannot create a new function that uses the deleted layer version. Any attempt to add the deleted layer version to a new function will fail.\n",
       "- If you need to update the layers used by the function, you will have to remove the reference to the deleted layer version and add a new layer version instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What architectures does Lambda support?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "According to the context provided, Lambda supports multiple instruction set architectures. Specifically, the context states that \"Lambda provides base images for each of the instruction set architectures and Lambda also provides base images that support both architectures.\" However, the context also notes that \"the image you build for your function must target only one of the architectures. Lambda does not support functions that use multi-architecture container images.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Questions about AWS Services"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What built-in algorithms are supported in SageMaker?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Request failed. Check the CloudWatch logs in your Lambda application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What languages are covered by Amazon Translate?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I don't have enough information to answer the question about the languages covered by Amazon Translate. The context provided is about AWS Lambda and does not mention anything about Amazon Translate or the languages it supports."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What file types does Amazon Kendra support?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Request failed. Check the CloudWatch logs in your Lambda application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Questions unrelated to AWS Services"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Which movie won an Oscar in 1995?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Request failed. Check the CloudWatch logs in your Lambda application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What are the differences between alligators and crocodiles?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Request failed. Check the CloudWatch logs in your Lambda application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What's the capital of Island?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Request failed. Check the CloudWatch logs in your Lambda application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/lab1_ex1_solutions.txt\n",
    "\n",
    "# These questions about Lambda are answered correctly\n",
    "lambda_questions = [\n",
    "    \"How is AWS Lambda different from EC2?\",\n",
    "    \"What happens to an AWS Lambda function if a Lambda layer is deleted?\",\n",
    "    \"What architectures does Lambda support?\"\n",
    "]\n",
    "\n",
    "# Most of these non-Lambda questions about AWS services lead to errors\n",
    "aws_questions = [\n",
    "    \"What built-in algorithms are supported in SageMaker?\",\n",
    "    \"What languages are covered by Amazon Translate?\",\n",
    "    \"What file types does Amazon Kendra support?\",\n",
    "]\n",
    "\n",
    "# These questions unrelated to AWS lead to errors\n",
    "unrelated_questions = [\n",
    "    \"Which movie won an Oscar in 1995?\",\n",
    "    \"What are the differences between alligators and crocodiles?\",\n",
    "    \"What's the capital of Island?\",\n",
    "]\n",
    "\n",
    "display(Markdown(\"### Questions about AWS Lambda\"))\n",
    "for lambda_q in lambda_questions:\n",
    "    display(Markdown(f\"**{lambda_q}**\"))\n",
    "    display(Markdown(make_live_request(lambda_q)))\n",
    "\n",
    "display(Markdown(\"### Questions about AWS Services\"))\n",
    "for aws_q in aws_questions:\n",
    "    display(Markdown(f\"**{aws_q}**\"))\n",
    "    display(Markdown(make_live_request(aws_q)))\n",
    "\n",
    "display(Markdown(\"### Questions unrelated to AWS Services\"))\n",
    "for unrelated_q in unrelated_questions:\n",
    "    display(Markdown(f\"**{unrelated_q}**\"))\n",
    "    display(Markdown(make_live_request(unrelated_q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "###### 2\n",
    "## <a> Part 2 - Explore LLM capabilities via prompting in Amazon Bedrock</a>\n",
    "([Go to top](#0))\n",
    "\n",
    "Let us now take a closer look at some of the foundational components and techniques that are being used in the GenAI application before extending it any further. In this section we will recreate the logic contained in the main code package and will experiment with modifications to it. \n",
    "\n",
    "Take a minute to inspect the code inside your `{Alias}MLUCourseLLMOps/src/{alias}_mlu_course_llm_ops/handler.py`. That contains the core logic to invoke the LLM that powers the GenAI application. \n",
    "\n",
    "You can quickly find the source code for that file in the link printed when you execute the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Click here to see the source code for the `handler.py` file"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://code.amazon.com/packages/KoachangMLUCourseLLMOps/blobs/mainline/--/src/koachang_mlu_course_llm_ops/handler.py\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### Click here to see the source code for the `handler.py` file\"))\n",
    "\n",
    "print(f\"https://code.amazon.com/packages/{MAIN_PACKAGE_NAME}/blobs/mainline/--/src/{alias}_mlu_course_llm_ops/handler.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating a scientist environment\n",
    "As mentioned in Module 1, __LLMOps foundations__, scientist roles typically work on environments that allow them to run their experiments comfortably, rather than operating directly in the systems that implement the solution in production.\n",
    "\n",
    "The next cells exemplify this idea by replicating code that is present in the Lambda function and breaking it out into pieces in this notebook, which simulates what might happen in an LLMOps process.\n",
    " \n",
    "A typical process would be for scientist roles to experiment and test in the notebook. When the model is ready, an SDE role would move this code into production Lambda functions and trigger the pipeline to create the new version. Ideally, a set of tests exist that any model change needs to pass.\n",
    "\n",
    "__Note__: for smaller teams, it is common that the same person executes both tasks, SDE an scientist, if they have the required skills for both.\n",
    "\n",
    "First, import the `langchain` library that you will use to simplify the calls to the Bedrock service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain.output_parsers.regex import RegexParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG with Amazon Kendra**\n",
    "\n",
    "The next cell refers to the RAG components of the application, built on Amazon Kendra. \n",
    "\n",
    "We get the Kendra index that is needed to retrieve  documents, then define a function that, given a query, returns the list of stored documents that are relevant to it. \n",
    "\n",
    "We will explore and extend this part of the system in **Lab 2: Enhancing the capabilities of the RAG system**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendra = session.client(\"kendra\", region_name=\"us-west-2\")\n",
    "response = kendra.list_indices()\n",
    "KENDRA_INDEX_ID = response[\"IndexConfigurationSummaryItems\"][0][\"Id\"]\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_context(query: str) -> dict:\n",
    "    \"\"\"Retrieve the list of documents from Kendra that are relevant to the query\"\"\"\n",
    "    response = kendra.retrieve(\n",
    "        IndexId=KENDRA_INDEX_ID,\n",
    "        QueryText=query,\n",
    "        PageNumber=1,\n",
    "        PageSize=5,\n",
    "    )\n",
    "    documents = response[\"ResultItems\"]\n",
    "    if(documents == []):\n",
    "        return {\n",
    "            \"question\": query,\n",
    "            \"context\": \"No relevant documents found\",\n",
    "        }\n",
    "    context = \"\\n\".join([document[\"Content\"] for document in documents])\n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"context\": context,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bedrock Guardrails for increased security**\n",
    "\n",
    "Next, let's see how the GenAI application incorporates [Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/) to implement safeguards that ensure the application complies with responsible AI policies. \n",
    "\n",
    "During the cloning of the GenAI application, a Bedrock guardrail was already created in your AWS Burner account. The AWS Lambda handler includes the guardrail to pre- and post-process all requests to the underlying LLM via the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "bedrock = session.client(\"bedrock\", region_name=\"us-west-2\")\n",
    "\n",
    "response = bedrock.list_guardrails()\n",
    "GUARDRAIL_ID = response[\"guardrails\"][0][\"id\"]\n",
    "GUARDRAIL_VERSION = response[\"guardrails\"][0][\"version\"]\n",
    "\n",
    "@tool(infer_schema=False)\n",
    "def guardrail(content: str) -> str:\n",
    "    \"\"\"Guard the content with Bedrock Guardrail. If the content is not flagged by Guardrail,\n",
    "    forward it to the next tool in chain.\n",
    "    \"\"\"\n",
    "    result = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=GUARDRAIL_ID,\n",
    "        guardrailVersion=GUARDRAIL_VERSION,\n",
    "        source=\"INPUT\",\n",
    "        content=[\n",
    "            {\n",
    "                \"text\": {\n",
    "                    \"text\": content,\n",
    "                    \"qualifiers\": [\n",
    "                        \"guard_content\",\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    if result[\"action\"] != \"NONE\":\n",
    "        print(f\"Guardrail ({GUARDRAIL_ID}) intervened ({result['ResponseMetadata']['RequestId']})\")\n",
    "        raise Exception(\"Content was blocked by guardrail\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see which configuration was used to create the available Bedrock Guardrail, run the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '0bfc6209-074d-4a47-aed9-dbc0424da990',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sat, 17 Aug 2024 23:53:01 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '763',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '0bfc6209-074d-4a47-aed9-dbc0424da990'},\n",
       "  'RetryAttempts': 0},\n",
       " 'name': 'KoachangMLUCourseLLMOpsGuardrail',\n",
       " 'guardrailId': 'xn0mzel53o7e',\n",
       " 'guardrailArn': 'arn:aws:bedrock:us-west-2:961341554577:guardrail/xn0mzel53o7e',\n",
       " 'version': 'DRAFT',\n",
       " 'status': 'READY',\n",
       " 'contentPolicy': {'filters': [{'type': 'PROMPT_ATTACK',\n",
       "    'inputStrength': 'HIGH',\n",
       "    'outputStrength': 'NONE'}]},\n",
       " 'sensitiveInformationPolicy': {'piiEntities': [{'type': 'AWS_ACCESS_KEY',\n",
       "    'action': 'BLOCK'},\n",
       "   {'type': 'AWS_SECRET_KEY', 'action': 'BLOCK'}],\n",
       "  'regexes': []},\n",
       " 'createdAt': datetime.datetime(2024, 8, 12, 20, 26, 4, tzinfo=tzlocal()),\n",
       " 'updatedAt': datetime.datetime(2024, 8, 12, 20, 26, 7, 945575, tzinfo=tzlocal()),\n",
       " 'statusReasons': [],\n",
       " 'failureRecommendations': [],\n",
       " 'blockedInputMessaging': 'PROMPT_INPUT_BLOCKED',\n",
       " 'blockedOutputsMessaging': 'MODEL_OUTPUT_BLOCKED'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock.get_guardrail(guardrailIdentifier=GUARDRAIL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see an example of an input that would be blocked by the GenAI application via the Bedrock Guardrail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koachang/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrail (xn0mzel53o7e) intervened (08678d1f-3b6b-427f-8dfc-47ea6397f644)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Content was blocked by guardrail",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_with_sensitive_pii \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms auth info for an AWS account: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mguardrail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_with_sensitive_pii\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/tools.py:740\u001b[0m, in \u001b[0;36mBaseTool.__call__\u001b[0;34m(self, tool_input, callbacks)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.47\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tool_input: \u001b[38;5;28mstr\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    739\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make tool callable.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/tools.py:615\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    614\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    616\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    617\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/tools.py:584\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    583\u001b[0m     tool_kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 584\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/tools.py:807\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    806\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 30\u001b[0m, in \u001b[0;36mguardrail\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNONE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrail (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGUARDRAIL_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) intervened (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponseMetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequestId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent was blocked by guardrail\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[0;31mException\u001b[0m: Content was blocked by guardrail"
     ]
    }
   ],
   "source": [
    "text_with_sensitive_pii = \"Here's auth info for an AWS account: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY.\"\n",
    "guardrail(text_with_sensitive_pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock to access multiple foundation models behind a common API\n",
    "\n",
    "With [Amazon Bedrock](https://aws.amazon.com/bedrock/), you can access generative AI foundation models from AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon via a single API. \n",
    "\n",
    "Although Bedrock models can be invoked via the [boto3 `invokeModel` API](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html), AWS integrations are available in the [langchain_aws](https://python.langchain.com/v0.2/docs/integrations/platforms/aws/) library that simplify the development of LLM-powered applications built with Amazon Bedrock. \n",
    "\n",
    "The code below initializes a Runnable interface to a Bedrock model. Your initial deployed GenAI application is powered by [Claude 3 Haiku](https://www.anthropic.com/news/claude-3-haiku), which is Anthropic's fastest and most affordable model in its class. \n",
    "\n",
    "Notice that the initialization sets some [inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) that influence the model's response. Read section [Use inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html#use-inference-parameters) to learn more about the effect of some commonly used parameters such as `temperature`, `top_k`, `top_p`, maximum new tokens, and end sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_claude_haiku = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    credentials_profile_name=\"MLU-LLMOps-Burner\",\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 1.0,\n",
    "    },\n",
    "    cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now copy the same prompt that the deployed application is using and can be found in the `handler.py` file on the main code package. Notice how the prompt contains placeholders for the `context` that will be retrieved from Kendra and the `question` from the user. \n",
    "\n",
    "The prompt also instructs the model to use the contextual information and to output its `answer` within proper XML tags, which is useful when parsing the response using LangChain's `RegexParser` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You act as an AWS Cloud Practitioner and only answer questions about AWS. Read the user's\n",
    "question supplied within the <question> tags. Then, use the contextual information provided\n",
    "above within the <context> tags to provide an answer in <answer> tag. Do not repeat the context.\n",
    "Respond that you don't know if you don't have enough information to answer\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\"\"\"\n",
    ")\n",
    "\n",
    "parser = RegexParser(regex=r\"(?s)<answer>(.*)</answer>\", output_keys=[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Langchain's syntax](https://python.langchain.com/docs/expression_language/get_started#basic-example-prompt-model-output-parser) it's easy to define a basic chain that sends the above prompt to the LLM and parses its response. \n",
    "\n",
    "In the code below we piece together all components into a single chain using the `|` symbol, akin to a Unix pipe operator. \n",
    "\n",
    "In our chain the following steps are concatenated: \n",
    " 1. The user input is first passed to the Bedrock Guardrail, that executes an `action` that might intercept, or not, the request, according to the defined configuration. \n",
    " 2. Next, the request is used to retrieve relevant context from the Kendra index, which will be used to enhance the LLM response using RAG. \n",
    " 3. Both user request and retrieved context are passed into the prompt template to generate the response from the LLM. \n",
    " 4. The generated model output is passed to the output parser. The output `parser` is defined as a regex that searches for text inside `<answer></answer>` tags. Notice that in this implementation, if the model's answer is not wrapped within `<answer></answer>` tags, the regex parser won't find any matches.\n",
    " 5. Finally, the parsed output is passed to the Bedrock Guardrail for potential flagging of the generated LLM response before reaching the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_claude_haiku = guardrail | retrieve_context | prompt | llm_claude_haiku | parser | guardrail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the chain, we will use the `invoke` method, passing any `question` from the user as input parameter. \n",
    "\n",
    "Below we define a function that takes a user question and a LangChain chain as input. The chain is invoked with the question and the answer is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, chain):\n",
    "    answer = chain.invoke(question)\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_challenge.png\" alt=\"MLU challenge\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b><p/>\n",
    "        <p><b>Exercise 2.</b> It is time to <b>experiment with Bedrock invocation</b>:</p>\n",
    "            <ul>\n",
    "                <li>Repeat some of the questions that you asked to the live system in <a href=\"#1\">Part 1</a>, but now invoking Bedrock directly with the <code>generate_answer</code> function.</li>\n",
    "                <li>Do the answers coincide with those from calling the deployed API endpoint? If not, why?</li>\n",
    "            </ul>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have enough information to answer when Elon Musk was born. The context provided is about AWS IAM roles and does not contain any information about Elon Musk's birth date."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "question = \"When was elon musk born?\"\n",
    "display(Markdown(generate_answer(question, chain_claude_haiku)))\n",
    "\n",
    "\n",
    "############# END OF CODE ###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        Setting temperature equal to zero does not guarantee deterministic response. \n",
    "    <br/><br/>\n",
    "        T=0 minimizes randomness in the model answer by choosing at every generation step the single most likely next token. However, it is believed that non determinism in GPU calculations around floating point operations might lead to divergent generations on occasion.\n",
    "    <br/><br/>If you're seeing discrepant responses between the live API endpoint and the <code style=\"color: lightcoral;\">generate_answer</code> function, that might be the reason.\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_question.png\" alt=\"MLU solution\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b><p/>\n",
    "        <p><b>Exercise 2.</b> Below we provide you with example questions to experiment with Bedrock invocation.</p>\n",
    "        <p>Remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display the sample solutions.</p>\n",
    "        <p>You can then re-run the cell to see its output.</p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What happens to an AWS Lambda function if a Lambda layer is deleted?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### API endpoint answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "If a Lambda layer that is being used by an AWS Lambda function is deleted, the following happens:\n",
       "\n",
       "- The Lambda function can continue to use the deleted layer version. The function will still be able to access and use the layer version that it was previously configured with.\n",
       "- However, you cannot create a new function that uses the deleted layer version. Any new functions or updates to the existing function's layer configuration will not be able to reference the deleted layer version.\n",
       "- If you need to update the layers used by the function, you will need to specify a different, non-deleted layer version. The function will continue to work as long as it can access at least one of the configured layer versions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Direct invocation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "If a Lambda layer that is being used by an AWS Lambda function is deleted, the following happens:\n",
       "\n",
       "- The Lambda function can continue to use the deleted layer version. The function will continue to run as if the layer version still exists.\n",
       "- However, you cannot create a new function that uses the deleted layer version. Any attempt to add the deleted layer version to a new function will fail.\n",
       "- If you need to update the layers used by the function, you will need to remove the reference to the deleted layer version and add a new version of the layer or a different layer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**What file types does Amazon Kendra support?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### API endpoint answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Request failed. Check the CloudWatch logs in your Lambda application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Direct invocation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output: I don't have enough information to answer the question about what file types Amazon Kendra supports. The context provided is about AWS IAM policies and access control, and does not contain any information about Amazon Kendra or the file types it supports.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m display(Markdown(make_live_request(question)))\n\u001b[1;32m     21\u001b[0m display(Markdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#### Direct invocation\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 22\u001b[0m display(Markdown(\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_claude_haiku\u001b[49m\u001b[43m)\u001b[49m))\n",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question, chain)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(question, chain):\n\u001b[0;32m----> 2\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2875\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2874\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2875\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    195\u001b[0m             config,\n\u001b[1;32m    196\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m         )\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1783\u001b[0m         Output,\n\u001b[0;32m-> 1784\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1792\u001b[0m     )\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:428\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    427\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 184\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    188\u001b[0m             config,\n\u001b[1;32m    189\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    195\u001b[0m             config,\n\u001b[1;32m    196\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m         )\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:237\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain/output_parsers/regex.py:35\u001b[0m, in \u001b[0;36mRegexParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     38\u001b[0m             key: text \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\n\u001b[1;32m     40\u001b[0m         }\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse output: I don't have enough information to answer the question about what file types Amazon Kendra supports. The context provided is about AWS IAM policies and access control, and does not contain any information about Amazon Kendra or the file types it supports."
     ]
    }
   ],
   "source": [
    "# %load solutions/lab1_ex2_solutions.txt\n",
    "\n",
    "# Notice how the answers to this question are essentially the same \n",
    "# regardless whether we send a request to the API endpoint or invoke Bedrock from this notebook\n",
    "question = \"What happens to an AWS Lambda function if a Lambda layer is deleted?\"\n",
    "display(Markdown(f\"**{question}**\"))\n",
    "\n",
    "display(Markdown(\"#### API endpoint answer\"))\n",
    "display(Markdown(make_live_request(question)))\n",
    "\n",
    "display(Markdown(\"#### Direct invocation\"))\n",
    "display(Markdown(generate_answer(question, chain_claude_haiku)))\n",
    "\n",
    "# Notice how the request fails when asking the following question\n",
    "question = \"What file types does Amazon Kendra support?\"\n",
    "display(Markdown(f\"**{question}**\"))\n",
    "\n",
    "display(Markdown(\"#### API endpoint answer\"))\n",
    "display(Markdown(make_live_request(question)))\n",
    "\n",
    "display(Markdown(\"#### Direct invocation\"))\n",
    "display(Markdown(generate_answer(question, chain_claude_haiku)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following solutions file provides more insight to diagnose the root problem that yields the application error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Output of LLM for first question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<answer>\\nIf a Lambda layer that is being used by an AWS Lambda function is deleted, the following happens:\\n\\n- The Lambda function can continue to use the deleted layer version. The function will continue to run as if the layer version still exists.\\n- However, you cannot create a new function that uses the deleted layer version. Any attempt to add the deleted layer version to a new function will fail.\\n- If you need to update the layers used by the function, you will have to remove the reference to the deleted layer version and add a new layer version instead.\\n</answer>' additional_kwargs={'usage': {'prompt_tokens': 1380, 'completion_tokens': 125, 'total_tokens': 1505}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'} response_metadata={'usage': {'prompt_tokens': 1380, 'completion_tokens': 125, 'total_tokens': 1505}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'} id='run-ecf9bed7-e9fc-4b69-b53b-120cd7b3e777-0' usage_metadata={'input_tokens': 1380, 'output_tokens': 125, 'total_tokens': 1505}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Output of LLM for second question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I don't have enough information to answer what file types Amazon Kendra supports. The context provided is about AWS IAM policies and access control, and does not contain any information about Amazon Kendra or the file types it supports.\" additional_kwargs={'usage': {'prompt_tokens': 1614, 'completion_tokens': 51, 'total_tokens': 1665}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'} response_metadata={'usage': {'prompt_tokens': 1614, 'completion_tokens': 51, 'total_tokens': 1665}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'} id='run-33cdf605-9aa5-4655-b046-99b0447b5876-0' usage_metadata={'input_tokens': 1614, 'output_tokens': 51, 'total_tokens': 1665}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Output of parser for second question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output: I don't have enough information to answer what file types Amazon Kendra supports. The context provided is about AWS IAM policies and access control, and does not contain any information about Amazon Kendra or the file types it supports.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m display(Markdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**Output of parser for second question**\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     18\u001b[0m chain \u001b[38;5;241m=\u001b[39m guardrail \u001b[38;5;241m|\u001b[39m retrieve_context \u001b[38;5;241m|\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm_claude_haiku \u001b[38;5;241m|\u001b[39m parser\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_2\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2875\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2874\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2875\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    195\u001b[0m             config,\n\u001b[1;32m    196\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m         )\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1783\u001b[0m         Output,\n\u001b[0;32m-> 1784\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1792\u001b[0m     )\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:428\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    427\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 184\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    188\u001b[0m             config,\n\u001b[1;32m    189\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    195\u001b[0m             config,\n\u001b[1;32m    196\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m         )\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:237\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/ws_KoachangMLUCourseLLMOps/src/KoachangMLUCourseLLMOpsExperiment/.venv/lib/python3.12/site-packages/langchain/output_parsers/regex.py:35\u001b[0m, in \u001b[0;36mRegexParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     38\u001b[0m             key: text \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\n\u001b[1;32m     40\u001b[0m         }\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse output: I don't have enough information to answer what file types Amazon Kendra supports. The context provided is about AWS IAM policies and access control, and does not contain any information about Amazon Kendra or the file types it supports."
     ]
    }
   ],
   "source": [
    "# %load solutions/lab1_ex2_solutions2.txt\n",
    "\n",
    "# Analyze the behaviour of the system for last question versus the first question\n",
    "question_1 = \"What happens to an AWS Lambda function if a Lambda layer is deleted?\"\n",
    "question_2 = \"What file types does Amazon Kendra support?\"\n",
    "\n",
    "# Notice what comes out of the LLM invocation for both questions:\n",
    "display(Markdown(\"**Output of LLM for first question**\"))\n",
    "chain = guardrail | retrieve_context | prompt | llm_claude_haiku\n",
    "print(chain.invoke(question_1))\n",
    "\n",
    "display(Markdown(\"**Output of LLM for second question**\"))\n",
    "chain = guardrail | retrieve_context | prompt | llm_claude_haiku\n",
    "print(chain.invoke(question_2))\n",
    "\n",
    "# Notice what happens when the output of the LLM goes through the parser\n",
    "display(Markdown(\"**Output of parser for second question**\"))\n",
    "chain = guardrail | retrieve_context | prompt | llm_claude_haiku | parser\n",
    "print(chain.invoke(question_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_challenge.png\" alt=\"MLU challenge\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b><p/>\n",
    "        <p><b>Exercise 3.</b> Now you can improve the way of invoking Bedrock models to try and fix errors returned by the application.</p>\n",
    "        <p>For the questions that led to a <code>Request failed</code> before, can you think of any modifications to fix the problem?</p>\n",
    "        Here're some ideas:\n",
    "            <ul>\n",
    "                <li><b>Modify the prompt</b> to force the correct behavior, providing examples if needed (few-shot prompting).</li>\n",
    "                <li>Experiment with <b>other models available in Bedrock</b> such as Claude 3 or the Mistral AI models.</li>\n",
    "                <li>Change the LangChain to <b>modify or remove the output parser</b>.</li>\n",
    "            </ul>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <b>LLM choice</b>\n",
    "    <br/><br/>\n",
    "        Your initially deployed GenAI application uses Claude 3 Haiku as a starting point and you can experiment with other models available in Amazon Bedrock. \n",
    "        <br/><br/>However, take into account cost and latency issues when choosing larger and/or more costly models. \n",
    "        <br/><br/>Your GenAI application deploys an endpoint to AWS Lambda and is thus subject to its timeout limits.\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############# END OF CODE ###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_question.png\" alt=\"MLU solution\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b><p/>\n",
    "        <p><b>Exercise 3.</b> Below we provide you with example solutions to fix errors in the Bedrock invocation.</p>\n",
    "        <p>Remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display the sample solutions.</p>\n",
    "        <p>You can then re-run the cell to see its output.</p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab1_ex3_solutions.txt\n",
    "\n",
    "# These questions about Lambda are answered correctly\n",
    "lambda_questions = [\n",
    "    \"How is AWS Lambda different from EC2?\",\n",
    "    \"What happens to an AWS Lambda function if a Lambda layer is deleted?\",\n",
    "    \"What architectures does Lambda support?\"\n",
    "]\n",
    "\n",
    "# Most of these non-Lambda questions about AWS services lead to errors\n",
    "aws_questions = [\n",
    "    \"What built-in algorithms are supported in SageMaker?\",\n",
    "    \"What languages are covered by Amazon Translate?\",\n",
    "    \"What file types does Amazon Kendra support?\",\n",
    "]\n",
    "\n",
    "# These questions unrelated to AWS lead to errors\n",
    "unrelated_questions = [\n",
    "    \"Which movie won an Oscar in 1995?\",\n",
    "    \"What are the differences between alligators and crocodiles?\",\n",
    "    \"What's the capital of Island?\",\n",
    "]\n",
    "\n",
    "all_questions = lambda_questions[:2] + aws_questions[:2] + unrelated_questions[:2]\n",
    "\n",
    "# This prompt for Claude 3 Haiku forces the model to follow a particular output format\n",
    "prompt_enforce = PromptTemplate.from_template(\n",
    "    \"\"\"You act as a AWS Cloud Practitioner and only answer questions about AWS. Read the user's\n",
    "question supplied within the <question></question> tags. Then, use the contextual information provided\n",
    "above within the <context></context> tags to provide an answer. Do not repeat the context.\n",
    "Respond that you don't know if you don't have enough information to answer.\n",
    "\n",
    "Return your output in <answer></answer> tags as in this example:\n",
    "\n",
    "<context>\n",
    "Example context\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "Example question\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "Example answer\n",
    "</answer>\n",
    "\n",
    "Below starts the real task:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\")\n",
    "\n",
    "chain_claude_haiku_enforce = guardrail | retrieve_context | prompt_enforce | llm_claude_haiku | parser | guardrail\n",
    "\n",
    "display(Markdown(\"## Direct invocation with Claude 3 Haiku and modified prompt\"))\n",
    "\n",
    "for question in all_questions:\n",
    "    display(Markdown(f\"**{question}**\"))\n",
    "    display(Markdown(generate_answer(question, chain_claude_haiku_enforce)))\n",
    "\n",
    "\n",
    "# This calls Claude 3 Sonnet instead of Haiku, without changing the original application prompt\n",
    "\n",
    "llm_claude_sonnet = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    credentials_profile_name=\"MLU-LLMOps-Burner\",\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 1.0,\n",
    "    },\n",
    "    cache=False,\n",
    ")\n",
    "\n",
    "chain_claude_sonnet = guardrail | retrieve_context | prompt | llm_claude_sonnet | parser | guardrail\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"## Direct invocation with Claude 3 Sonnet and original prompt\"))\n",
    "\n",
    "for question in all_questions:\n",
    "    display(Markdown(f\"**{question}**\"))\n",
    "    display(Markdown(generate_answer(question, chain_claude_sonnet)))\n",
    "\n",
    "\n",
    "# This calls model Mistral AI with a Mistral-specific prompt and removes the output parser from the chain\n",
    "\n",
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "llm_mistral = BedrockLLM(\n",
    "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
    "    credentials_profile_name=\"MLU-LLMOps-Burner\",\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 1.0,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    },\n",
    "    cache=False,\n",
    ")\n",
    "\n",
    "\n",
    "prompt_mistral = PromptTemplate.from_template(\n",
    "    \"\"\"[INST]You act as a AWS Cloud Practitioner and only answer questions about AWS. Read the user's\n",
    "question supplied within the <question> tags. Then, use the contextual information provided\n",
    "above within the <context> tags to provide an answer. Do not repeat the context.\n",
    "Respond that you don't know if you don't have enough information to answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# This chain removes the output parser\n",
    "chain_mistral = guardrail | retrieve_context | prompt_mistral | llm_mistral | guardrail\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"## Direct invocation with Mistral 7b and modified prompt\"))\n",
    "\n",
    "# Run only a subset of all questions to avoid Throttling error\n",
    "for question in all_questions[2:]:\n",
    "    display(Markdown(f\"**{question}**\"))\n",
    "    display(Markdown(generate_answer(question, chain_mistral)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3\n",
    "## <a>Part 3 - Implement changes to the system and deploy with CDK</a>\n",
    "([Go to top](#0))\n",
    "\n",
    "After experimenting with prompts, models, and other parameters, you might come up with an improved way to invoke the LLM in your system. Eventually, it is time to deploy those changes so that they're available via the API endpoint!\n",
    "\n",
    "The ultimate goal is to update the CloudFormation template behind the deployed AWS Lambda service. A CloudFormation stack update is triggered by a call to the CloudFormation API with a new template and artifacts such as code assets or data.\n",
    "\n",
    "There are serveral ways to trigger such CloudFormation API. Two common ways are:\n",
    "- using Pipelines\n",
    "- deploying locally via CDK CLI\n",
    "\n",
    "Both have the same effect. Deploying via Pipelines is recommended for a real workload, as it commits lasting changes to the code repositories. Deploying using CDK is temporary and the deployment will eventually be overridden by a Pipelines deployment later, but can be useful for fast tests.\n",
    "\n",
    "For this first change we will use local deployment via CDK CLI for faster turnaround. Read [How the Brazil CDK Pipelines Work](https://builderhub.corp.amazon.com/docs/pipelines/cdk-guide/concepts-cdk-pipelines.html) if you want to know more about CDK.\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_challenge.png\" alt=\"MLU challenge\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <br/><p><b>Try it yourself!</b><p/>\n",
    "        <p><b>Exercise 4.</b> After experimenting and testing changes to the LLM interaction in this playground notebook, you can now make changes to the live service by modifying the code that is run by the AWS Lambda function and using the CDK package to deploy the service stack.</p>\n",
    "        <p>\n",
    "        To do so, follow these steps: \n",
    "        <ol>\n",
    "            <li>Using VS Code or a Terminal in your Cloud Desktop, navigate to folder <code>{Alias}MLUCourseLLMOps</code> and locate file <code>{Alias}MLUCourseLLMOps/src/{alias}_mlu_course_llm_ops/handler.py</code>.</li><br/>\n",
    "            <li>Edit the file <code>{Alias}MLUCourseLLMOps/src/{alias}_mlu_course_llm_ops/handler.py</code> with your desired changes, for instance the <code>prompt</code> to the Bedrock model, the actual model, or the chain. This is where you <b>bring the results of your experimentation in the notebook to the actual production code</b>.</li><br/>\n",
    "            <li>Save the edited file.</li><br/>\n",
    "            <li>From a terminal in your Cloud Desktop, build the main application package. To do so, execute the cell below to get the exact command that you need to run in the command line to trigger the build.</li>\n",
    "        </ol>\n",
    "        </p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Copy these commands and run them on your Cloud Desktop terminal to build the main application package."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd $(brazil-context workspace root)/src/KoachangMLUCourseLLMOps\n",
      "brazil-build release\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### Copy these commands and run them on your Cloud Desktop terminal to build the main application package.\"))\n",
    "\n",
    "print(f\"cd $(brazil-context workspace root)/src/{MAIN_PACKAGE_NAME}\")\n",
    "\n",
    "print(f\"brazil-build release\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_challenge.png\" alt=\"MLU challenge\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <br/><br/><p>Continue with these steps:<p/>\n",
    "        <ol start=\"5\">\n",
    "            <li>Switch to the CDK package and deploy the service stack. To do that, execute the cell below to get the exact command that you need run in the command line to perform the deployment.</li><br/>\n",
    "        </ol>\n",
    "        </p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Copy these commands and run them on your Cloud Desktop terminal to deploy the service stack."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd $(brazil-context workspace root)/src/KoachangMLUCourseLLMOpsCDK\n",
      "brazil-build && brazil-build run cdk deploy KoachangMLUCourseLLMOps-Service-alpha -- --hotswap\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### Copy these commands and run them on your Cloud Desktop terminal to deploy the service stack.\"))\n",
    "\n",
    "print(f\"cd $(brazil-context workspace root)/src/{MAIN_PACKAGE_NAME}CDK\")\n",
    "\n",
    "print(f\"brazil-build && brazil-build run cdk deploy {MAIN_PACKAGE_NAME}-Service-alpha -- --hotswap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_challenge.png\" alt=\"MLU challenge\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <br/><p>Continue with these steps:<p/>\n",
    "        <ol start=\"6\">\n",
    "            <li>Wait for the cdk deploy command to complete.</li><br/>\n",
    "            <li><b>Now ask the same questions from Part 1. Does the behavior from the live system change?</b></li><br/>If done properly, your fix should now prevent any errors coming back from the live system.\n",
    "        </ol>\n",
    "        </p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############# END OF CODE ###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_question.png\" alt=\"MLU solution\" width=15% height=15%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b><p/>\n",
    "        <p><b>Exercise 4.</b> Below we provide you with example solutions to call the current live service.</p>\n",
    "        <p>Remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display the sample solutions.</p>\n",
    "        <p>You can then re-run the cell to see its output.</p>\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab1_ex4_solutions.txt\n",
    "\n",
    "# These questions about Lambda are answered correctly\n",
    "lambda_questions = [\n",
    "    \"How is AWS Lambda different from EC2?\",\n",
    "    \"What happens to an AWS Lambda function if a Lambda layer is deleted?\",\n",
    "    \"What architectures does Lambda support?\"\n",
    "]\n",
    "\n",
    "# Most of these non-Lambda questions about AWS services lead to errors\n",
    "aws_questions = [\n",
    "    \"What built-in algorithms are supported in SageMaker?\",\n",
    "    \"What languages are covered by Amazon Translate?\",\n",
    "    \"What file types does Amazon Kendra support?\",\n",
    "]\n",
    "\n",
    "# These questions unrelated to AWS lead to errors\n",
    "unrelated_questions = [\n",
    "    \"Which movie won an Oscar in 1995?\",\n",
    "    \"What are the differences between alligators and crocodiles?\",\n",
    "    \"What's the capital of Island?\",\n",
    "]\n",
    "\n",
    "all_questions = lambda_questions + aws_questions + unrelated_questions\n",
    "\n",
    "# This runs all questions from above against the current live service\n",
    "for question in all_questions:\n",
    "    display(Markdown(f\"**{question}**\"))\n",
    "    display(Markdown(make_live_request(question)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 1 of MLU's course Operationalizing Generative AI with LLMOps.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

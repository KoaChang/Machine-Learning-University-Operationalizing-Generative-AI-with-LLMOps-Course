{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "is_executing": true
            },
            "outputs": [],
            "source": [
                "import logging\n",
                "import boto3\n",
                "import json\n",
                "import pandas as pd\n",
                "import subprocess\n",
                "import nltk\n",
                "import lancedb\n",
                "from IPython.display import display, HTML\n",
                "from ipywidgets import IntProgress\n",
                "from amzn_personal_playground import (\n",
                "    Model,\n",
                "    cosine_similarity,\n",
                "    rouge_score,\n",
                ")\n",
                "from datasets import load_dataset\n",
                "\n",
                "from langchain.document_loaders import TextLoader\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from langchain.output_parsers.regex import RegexParser\n",
                "from langchain_aws import ChatBedrock\n",
                "from langchain_community.llms import Bedrock\n",
                "from langchain.prompts import PromptTemplate\n",
                "from langchain.embeddings import BedrockEmbeddings\n",
                "\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "\n",
                "# Initialize SDK clients, in case we want to interact with Bedrock directly instead of using LangChain\n",
                "session = boto3.Session(profile_name=\"bedrock\")\n",
                "bedrock_runtime = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
                "# Uncomment this line and use this client for Titan model. You must be in VPN or Amazon's CORP network to run this line.\n",
                "# bedrock_runtime_private = session.client(\"bedrock-runtime\", region_name=\"us-west-2\", endpoint_url=\"https://prod.us-west-2.dataplane.bedrock.aws.dev\")\n",
                "\n",
                "nltk.download(\"punkt\")\n",
                "nltk.download(\"stopwords\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Personal playground\n",
                "\n",
                "Demonstrating basic interaction with Bedrock models, using either AWS SDK directly or LangChain. For each option, we are testing three different scenarios: prompting without a context, prompting with a static context, and prompting with context pulled from a vector store. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "context = \"\"\"The instruction set architecture of a Lambda function determines the type of computer\n",
                "processor that Lambda uses to run the function. Lambda provides a choice of instruction set\n",
                "architectures:\n",
                "    arm64 – 64-bit ARM architecture, for the AWS Graviton2 processor.\n",
                "    x86_64 – 64-bit x86 architecture, for x86-based processors.\n",
                "\"\"\"\n",
                "\n",
                "question = \"What architectures does Lambda support?\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false
            },
            "source": [
                "## Using AWS SDK directly"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt without a context\n",
                "\n",
                "Without a helpful context and instructions, there's good chance that the answers from LLM will be inaccurate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "prompt = \"\"\"Human:\n",
                "\n",
                "{question}\n",
                "\n",
                "Assistant:\n",
                "\n",
                "\"\"\"\n",
                "\n",
                "response = bedrock_runtime.invoke_model(\n",
                "    body=json.dumps({\n",
                "        \"prompt\": prompt.format(question=question),\n",
                "        \"max_tokens_to_sample\": 4096,\n",
                "        \"temperature\": 0.0,\n",
                "        \"top_k\": 10,\n",
                "        \"top_p\": 1.0,\n",
                "    }),\n",
                "    modelId=Model.ANTHROPIC_CLAUDE_INSTANT.value,\n",
                ")\n",
                "print(json.loads(response.get(\"body\").read())[\"completion\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt with proper context and prompt engineering\n",
                "\n",
                "With a relevant context included in the prompt, the LLM should be able to provide an accurate answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"\"\"Human:\n",
                "\n",
                "You act as an AWS Cloud Practitioner and only answer questions about AWS. Read the user’s question\n",
                "supplied within the <question> tags. Then, use the contextual information provided above within the\n",
                "<context> tags to provide an answer in <answer> tag. Do not repeat the context. Respond that you\n",
                "don't know if you don't have enough information  to answer.\n",
                "\n",
                "<context>\n",
                "{context}\n",
                "</context>\n",
                "\n",
                "<question>\n",
                "{question}\n",
                "</question>\n",
                "\n",
                "Assistant:\n",
                "\n",
                "\n",
                "\"\"\"\n",
                "\n",
                "response = bedrock_runtime.invoke_model(\n",
                "    body=json.dumps({\n",
                "        \"prompt\": prompt.format(question=question, context=context),\n",
                "        \"max_tokens_to_sample\": 4096,\n",
                "        \"temperature\": 0.0,\n",
                "        \"top_k\": 10,\n",
                "        \"top_p\": 1.0,\n",
                "    }),\n",
                "    modelId=Model.ANTHROPIC_CLAUDE_INSTANT.value,\n",
                ")\n",
                "print(json.loads(response.get(\"body\").read())[\"completion\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt with Retrieval-augmented generation (RAG)\n",
                "\n",
                "RAG allows pulling contexts (that are relevant to the provided question) from a vector store. A typical RAG application has two main components:\n",
                "\n",
                "1. Indexing\n",
                "  * Load: First we need to load our data. We'll use DocumentLoaders for this.\n",
                "  * Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't in a model's finite context window.\n",
                "  * Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
                "\n",
                "2. Querying\n",
                "  * Retrieve context: Given a user input, relevant splits are retrieved from storage using a Retriever.  \n",
                "  * Generate from context: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"\"\"Human:\n",
                "You act as an AWS Cloud Practitioner and only answer questions about AWS. Read the user’s question\n",
                "supplied within the <question> tags. Then, use the contextual information provided above within the\n",
                "<context> tags to provide an answer in <answer> tag. Do not repeat the context. Respond that you\n",
                "don't know if you don't have enough information  to answer.\n",
                "\n",
                "<context>\n",
                "{context}\n",
                "</context>\n",
                "\n",
                "<question>\n",
                "{question}\n",
                "</question>\n",
                "\n",
                "Assistant:\n",
                "\n",
                "\n",
                "\"\"\"\n",
                "\n",
                "# Indexing - load\n",
                "package_root = subprocess.run([\"brazil-context\", \"package\", \"root\"], capture_output=True).stdout.decode().strip()\n",
                "loader = TextLoader(f\"{package_root}/example_data/lambda_deploy_to_multiple_architectures.txt\")\n",
                "data = loader.load()\n",
                "\n",
                "# Indexing - split\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
                "all_splits = text_splitter.split_documents(data)\n",
                "\n",
                "# Indexing - store: converting texts to embeddings\n",
                "response = bedrock_runtime.invoke_model(\n",
                "    body=json.dumps({\n",
                "        \"texts\": list(map(lambda x: x.page_content, all_splits)),\n",
                "        # input type of \"search_document\" is used when we want to generate embeddings\n",
                "        # to store in a vector database for search use-cases.\n",
                "        \"input_type\": \"search_document\",\n",
                "    }),\n",
                "    modelId=Model.COHERE_EMBED_ENGLISH.value,\n",
                ")\n",
                "embeddings = json.loads(response.get(\"body\").read())[\"embeddings\"]\n",
                "\n",
                "# Indexing - store: storing embeddings\n",
                "vector_db = lancedb.connect(\"/tmp/lancedb\")\n",
                "data = [{\n",
                "    \"vector\": embeddings[i],\n",
                "    \"text\": all_splits[i].page_content,\n",
                "    \"id\": i,\n",
                "} for i in range(len(all_splits))]\n",
                "vector_table = vector_db.create_table(\"rag-data\", data=data, mode=\"overwrite\")\n",
                "\n",
                "# Querying - retrieve context\n",
                "response = bedrock_runtime.invoke_model(\n",
                "    body=json.dumps({\n",
                "        \"texts\": [question],\n",
                "        # input type of \"search_document\" is used when we want to generate embeddings\n",
                "        # to store in a vector database for search use-cases.\n",
                "        \"input_type\": \"search_document\",\n",
                "    }),\n",
                "    modelId=Model.COHERE_EMBED_ENGLISH.value,\n",
                ")\n",
                "question_embedding = json.loads(response.get(\"body\").read())[\"embeddings\"][0]\n",
                "relevant_chunks = vector_table.search(question_embedding)\n",
                "relevant_chunk_ids = [chunk[\"id\"] for chunk in relevant_chunks.to_list()]\n",
                "\n",
                "# Querying - generate content\n",
                "context = \"\\n\".join([\n",
                "    all_splits[int(split_id)].page_content\n",
                "    for split_id in relevant_chunk_ids\n",
                "])\n",
                "\n",
                "\n",
                "response = bedrock_runtime.invoke_model(\n",
                "    body=json.dumps({\n",
                "        \"prompt\": prompt.format(question=question, context=context),\n",
                "        \"max_tokens_to_sample\": 4096,\n",
                "        \"temperature\": 0.0,\n",
                "        \"top_k\": 10,\n",
                "        \"top_p\": 1.0,\n",
                "    }),\n",
                "    modelId=Model.ANTHROPIC_CLAUDE_INSTANT.value,\n",
                ")\n",
                "print(json.loads(response.get(\"body\").read())[\"completion\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using LangChain"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setting up the chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "llm_claude = Bedrock(\n",
                "    credentials_profile_name=\"bedrock\",\n",
                "    model_id=Model.ANTHROPIC_CLAUDE_INSTANT.value,\n",
                "    region_name=\"us-west-2\",\n",
                "    model_kwargs={\n",
                "        \"max_tokens_to_sample\": 500,\n",
                "        \"temperature\": 0.1,\n",
                "        \"top_k\": 10,\n",
                "        \"top_p\": 1.0,\n",
                "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
                "    },\n",
                "    cache=False\n",
                ")\n",
                "llm_cohere_embed = BedrockEmbeddings(\n",
                "    credentials_profile_name=\"bedrock\",\n",
                "    model_id=Model.COHERE_EMBED_ENGLISH.value,\n",
                "    region_name=\"us-west-2\",\n",
                ")\n",
                "\n",
                "parser = RegexParser(regex=r\"(?s)<answer>(.*)</answer>\", output_keys=[\"answer\"])\n",
                "\n",
                "prompt = PromptTemplate(\n",
                "    template=\"\"\"\n",
                "You act as an AWS Cloud Practitioner and only answer questions about AWS. Read the user’s question\n",
                "supplied within the <question> tags. Then, use the contextual information provided above within the\n",
                "<context> tags to provide an answer in <answer> tag. Do not repeat the context. Respond that you\n",
                "don't know if you don't have enough information to answer.\n",
                "\n",
                "<context>\n",
                "{context}\n",
                "</context>\n",
                "\n",
                "<question>\n",
                "{question}\n",
                "</question>\n",
                "\"\"\",\n",
                "    input_variables=[\"question\", \"context\"]\n",
                ")\n",
                "\n",
                "chain_claude = prompt | llm_claude | parser\n",
                "chain_cohere_embed = llm_cohere_embed\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt without context"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "print(chain_claude.invoke({ \"question\": question, \"context\": \"\" })[\"answer\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt with context\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "context = \"\"\"The instruction set architecture of a Lambda function determines the type of computer\n",
                "processor that Lambda uses to run the function. Lambda provides a choice of instruction set\n",
                "architectures:\n",
                "    arm64 – 64-bit ARM architecture, for the AWS Graviton2 processor.\n",
                "    x86_64 – 64-bit x86 architecture, for x86-based processors.\n",
                "\"\"\"\n",
                "\n",
                "print(chain_claude.invoke({ \"question\": question, \"context\": context })[\"answer\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prompt with Retrieval-augmented generation (RAG)\n",
                "\n",
                "A typical RAG application has two main components:\n",
                "\n",
                "1. Indexing\n",
                "  * Load: First we need to load our data. We'll use DocumentLoaders for this.\n",
                "  * Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't in a model's finite context window.\n",
                "  * Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
                "\n",
                "2. Retrieval and generation\n",
                "  * Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.  \n",
                "  * Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Indexing - load\n",
                "package_root = subprocess.run([\"brazil-context\", \"package\", \"root\"], capture_output=True).stdout.decode().strip()\n",
                "loader = TextLoader(f\"{package_root}/example_data/lambda_deploy_to_multiple_architectures.txt\")\n",
                "data = loader.load()\n",
                "\n",
                "# Indexing - split\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
                "all_splits = text_splitter.split_documents(data)\n",
                "\n",
                "# Indexing - store\n",
                "embeddings = chain_cohere_embed.embed_documents(list(map(lambda x: x.page_content, all_splits)))\n",
                "\n",
                "# Indexing - store: storing embeddings\n",
                "vector_db = lancedb.connect(\"/tmp/lancedb\")\n",
                "data = [{\n",
                "    \"vector\": embeddings[i],\n",
                "    \"text\": all_splits[i].page_content,\n",
                "    \"id\": i,\n",
                "} for i in range(len(all_splits))]\n",
                "vector_table = vector_db.create_table(\"rag-data\", data=data, mode=\"overwrite\")\n",
                "\n",
                "# Querying - retrieve context\n",
                "question_embedding = chain_cohere_embed.embed_query(question)\n",
                "relevant_chunks = vector_table.search(question_embedding)\n",
                "relevant_chunk_ids = [chunk[\"id\"] for chunk in relevant_chunks.to_list()]\n",
                "\n",
                "# Querying - generate content\n",
                "context = \"\\n\".join([\n",
                "    all_splits[int(split_id)].page_content\n",
                "    for split_id in relevant_chunk_ids\n",
                "])\n",
                "print(chain_claude.invoke({ \"question\": question, \"context\": context })[\"answer\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluation\n",
                "\n",
                "Evaluating Claude 3 Haiku, Jurassic Mid, and Titan Express models on the ability to summarize texts. The dataset being used is \n",
                "[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) but we should be able to swap it out to \n",
                "a custom dataset -- just load your own data to the `articles` and `ground_truths` lists. \n",
                "\n",
                "For each model, we compare their responses with the ground truth to generate ROUGEL score and Cosine \n",
                "Similarity metrics. We use Claude-as-a-judge to assess the accuracy, coherence, factuality, and \n",
                "completeness of the summary."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preparing evaluation datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluation_dataset_size = 10\n",
                "\n",
                "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
                "\n",
                "evaluation_dataset = list(dataset[\"train\"])[:evaluation_dataset_size]\n",
                "articles = [entry[\"article\"] for entry in evaluation_dataset]\n",
                "ground_truths = [entry[\"highlights\"] for entry in evaluation_dataset]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting up prompts and chains"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = PromptTemplate.from_template(\"\"\"\n",
                "Summarize the content that is wrapped in <article> tag. Output the summary within a <summary> tag.\n",
                "\n",
                "<article>\n",
                "{text}\n",
                "</article>\n",
                "\"\"\"\n",
                ")\n",
                "\n",
                "judge_prompt = PromptTemplate.from_template(\"\"\"\n",
                "You will be given a summary of a news article. Your task is to evaluate the summary in four dimensions:\n",
                "accuracy, coherence, factuality, and completeness. Provide a score of 1-5 in each dimension, with 5\n",
                "being the best score. If the summary does not contain any useful summarization, give it a score of 0.\n",
                "\n",
                "Original article: {article}\n",
                "\n",
                "Summary: {summarization}\n",
                "\n",
                "Write the scores to the corresponding tags as below:\n",
                "\n",
                "<coherence></coherence>\n",
                "<accuracy></accuracy>\n",
                "<factuality></factuality>\n",
                "<completeness></factuality>\n",
                "\"\"\")\n",
                "\n",
                "# Claude 3 uses Messages API instead of Text Completion API, so it requires a different chain\n",
                "# (ChatBedrock instead of Bedrock)\n",
                "llm_claude = ChatBedrock(\n",
                "    credentials_profile_name=\"bedrock\",\n",
                "    model_id=Model.ANTHROPIC_CLAUDE_3_HAIKU.value,\n",
                "    region_name=\"us-west-2\",\n",
                "    model_kwargs={\n",
                "        \"max_tokens\": 4096,\n",
                "        \"temperature\": 0.1,\n",
                "        \"top_k\": 10,\n",
                "        \"top_p\": 1.0,\n",
                "    },\n",
                "    cache=False\n",
                ")\n",
                "\n",
                "llm_jurassic = Bedrock(\n",
                "    credentials_profile_name=\"bedrock\",\n",
                "    model_id=Model.AI21_JURASSIC_MID.value,\n",
                "    region_name=\"us-west-2\",\n",
                "    model_kwargs={\n",
                "        \"maxTokens\": 4096,\n",
                "        \"temperature\": 0.1,\n",
                "        \"countPenalty\": {\"scale\": 0},\n",
                "        \"presencePenalty\": {\"scale\": 0},\n",
                "        \"frequencyPenalty\": {\"scale\": 0},\n",
                "    },\n",
                "    cache=False\n",
                ")\n",
                "\n",
                "llm_titan = Bedrock(\n",
                "    credentials_profile_name=\"bedrock\",\n",
                "    model_id=Model.AMAZON_TITAN_TEXT_EXPRESS.value,\n",
                "    region_name=\"us-west-2\",\n",
                "    model_kwargs={\n",
                "        \"maxTokenCount\": 4096,\n",
                "        \"temperature\": 0.1,\n",
                "    },\n",
                "    cache=False\n",
                ")\n",
                "\n",
                "\n",
                "parser = RegexParser(regex=r\"(?s)<summary>(.*)</summary>\", output_keys=[\"summary\"])\n",
                "chain_claude = prompt | llm_claude | parser\n",
                "chain_jurassic = prompt | llm_jurassic\n",
                "\n",
                "chain_titan = prompt | llm_titan\n",
                "\n",
                "evaluation_parser = RegexParser(\n",
                "    regex=r\"(?s)<coherence>(.*?)</coherence>.*<accuracy>(.*?)</accuracy>.*<factuality>(.*?)</factuality>.*<completeness>(.*?)</completeness>\",\n",
                "    output_keys=[\"coherence\", \"accuracy\", \"factuality\", \"completeness\"],\n",
                ")\n",
                "chain_judge_claude = judge_prompt | llm_claude | evaluation_parser\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inferencing (querying models)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "\n",
                "progress = IntProgress(min=0, max=len(articles))\n",
                "progress.value = 0\n",
                "display(progress)\n",
                "for idx, article in enumerate(articles):\n",
                "    # Generate summaries\n",
                "    claude_summary = chain_claude.invoke({\"text\": article})[\"summary\"]\n",
                "    jurassic_summary = chain_jurassic.invoke({\"text\": article})\n",
                "    titan_summary = chain_titan.invoke({\"text\": article})\n",
                "\n",
                "    ground_truth = ground_truths[idx]\n",
                "\n",
                "    result = [\n",
                "        article,\n",
                "        ground_truth,\n",
                "        claude_summary,\n",
                "        jurassic_summary,\n",
                "        titan_summary,\n",
                "    ]\n",
                "\n",
                "    results.append(result)\n",
                "\n",
                "    progress.value += 1\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Scoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "progress = IntProgress(min=0, max=len(articles))\n",
                "progress.value = 0\n",
                "display(progress)\n",
                "\n",
                "for result in results:\n",
                "    article, ground_truth, claude_summary, jurassic_summary, titan_summary = result\n",
                "\n",
                "    claude_scores = chain_judge_claude.invoke({\"article\": article, \"summarization\": claude_summary})\n",
                "    jurassic_scores = chain_judge_claude.invoke({\"article\": article, \"summarization\": jurassic_summary})\n",
                "    titan_scores = chain_judge_claude.invoke({\"article\": article, \"summarization\": titan_summary})\n",
                "    result.extend([\n",
                "        int(claude_scores['accuracy']),\n",
                "        int(claude_scores['coherence']),\n",
                "        int(claude_scores['factuality']),\n",
                "        int(claude_scores['completeness']),\n",
                "    ])\n",
                "    result.append(cosine_similarity(ground_truth, claude_summary))\n",
                "    result.append(rouge_score(\"rougeL\", ground_truth, claude_summary))\n",
                "\n",
                "    result.extend([\n",
                "        int(jurassic_scores['accuracy']),\n",
                "        int(jurassic_scores['coherence']),\n",
                "        int(jurassic_scores['factuality']),\n",
                "        int(jurassic_scores['completeness']),\n",
                "    ])\n",
                "    result.append(cosine_similarity(ground_truth, jurassic_summary))\n",
                "    result.append(rouge_score(\"rougeL\", ground_truth, jurassic_summary))\n",
                "\n",
                "    result.extend([\n",
                "        int(titan_scores['accuracy']),\n",
                "        int(titan_scores['coherence']),\n",
                "        int(titan_scores['factuality']),\n",
                "        int(titan_scores['completeness']),\n",
                "    ])\n",
                "    result.append(cosine_similarity(ground_truth, titan_summary))\n",
                "    result.append(rouge_score(\"rougeL\", ground_truth, titan_summary))\n",
                "\n",
                "    progress.value += 1\n",
                "\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "df.columns = [\n",
                "    'article',\n",
                "    'ground_truth',\n",
                "    'claude_summary',\n",
                "    'jurassic_summary',\n",
                "    'titan_summary',\n",
                "\n",
                "    'claude_accuracy',\n",
                "    'claude_coherence',\n",
                "    'claude_factuality',\n",
                "    'claude_completeness',\n",
                "    'claude_cosine_similarity',\n",
                "    'claude_rouge_l',\n",
                "\n",
                "    'jurassic_accuracy',\n",
                "    'jurassic_coherence',\n",
                "    'jurassic_factuality',\n",
                "    'jurassic_completeness',\n",
                "    'jurassic_cosine_similarity',\n",
                "    'jurassic_rouge_l',\n",
                "\n",
                "    'titan_accuracy',\n",
                "    'titan_coherence',\n",
                "    'titan_factuality',\n",
                "    'titan_completeness',\n",
                "    'titan_cosine_similarity',\n",
                "    'titan_rouge_l',\n",
                "]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Flock for scoring\n",
                "\n",
                "Read https://builderhub.corp.amazon.com/docs/gen-ai/golden-path-genai-model-evaluation.html#use-flock-for-evaluation-task-locally-in-your-development-environment for more details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from flock_eval.evaluation import GroundTruthTextBasedEvaluator\n",
                "from flock_eval.logging import logger\n",
                "from flock_eval.testsuite import TextTestCase\n",
                "from flock_eval.similarity.config import BedrockMetricConfig, MetricConfig, MetricDefinitions\n",
                "\n",
                "# See evaluation metrics definitions in https://code.amazon.com/packages/FlockEval/blobs/mainline/--/doc/evaluation.md\n",
                "metrics = MetricDefinitions(\n",
                "    # Flock uses 'rougeLsum' metric. ROUGE-LSum is a specialized version of ROUGE-L, fine-tuned to better evaluate the \n",
                "    # quality of summaries by considering the unique characteristics of summarization.\n",
                "    # 1. Evaluates at the level of sentences instead of the entire document.\n",
                "    # 2. Captures the main ideas in a condensed form is more critical than preserving the entire structure of the original text.\n",
                "    rouge=MetricConfig(enabled=True),\n",
                "    \n",
                "    # This metric is computed by invoking an LLM to find out the similarity between two texts. \n",
                "    llm=BedrockMetricConfig(enabled=True, bedrock_model_id=Model.ANTHROPIC_CLAUDE_3_HAIKU.value),\n",
                "    \n",
                "    # The LLM_Correctness metric allows the LLM to indicate whether or not it thinks the system response is better or \n",
                "    # worse than the provided ground truth response, in terms of correctness.\n",
                "    llm_correctness=BedrockMetricConfig(enabled=True, bedrock_model_id=Model.ANTHROPIC_CLAUDE_3_HAIKU.value),\n",
                "    \n",
                "    # A very basic similarity check that checks for an exact match only. It is not suitable for our use case.\n",
                "    binary=MetricConfig(enabled=False),\n",
                "    \n",
                "    # Typically used for machine translation evaluation. It accounts for rephrasing by using traditional NLP techniques \n",
                "    # such as stemming. It also includes a penalty for differences in word order between the candidate and the \n",
                "    # reference. This helps ensure that translations are not only accurate in terms of content but also in their \n",
                "    # syntactic structure.\n",
                "    # Flock does not support latest NLTK METEOR metric. Tracking in https://t.corp.amazon.com/P139792093. \n",
                "    meteor=MetricConfig(enabled=False),\n",
                "    \n",
                "    # Computes semantic similarity (cosine similarity between embedding vectors). This distance may be better suited to \n",
                "    # sentences. A longer text may be truncated.\n",
                "    semantic=BedrockMetricConfig(enabled=True, bedrock_model_id=Model.AMAZON_TITAN_EMBED_TEXT.value),\n",
                ")\n",
                "\n",
                "evaluator = GroundTruthTextBasedEvaluator(metrics)\n",
                "\n",
                "# Update the evaluator to use bedrock runtime client created with 'bedrock' credential profile\n",
                "rating_suite = evaluator.rating_suite(metrics)\n",
                "rating_suite._enabled_metrics['llm'].llm.client = bedrock_runtime\n",
                "rating_suite._enabled_metrics['llm_correctness'].llm.client = bedrock_runtime\n",
                "rating_suite._enabled_metrics['semantic'].embedding_model.client = bedrock_runtime\n",
                "\n",
                "def evaluate_with_flock(inference_df, prediction_column_name):\n",
                "    dataset_in_flock_format = inference_df[['article', 'ground_truth', prediction_column_name]]\\\n",
                "        .rename(columns={\n",
                "            'article': 'input', \n",
                "            'ground_truth': 'ground_truth_solution', \n",
                "            prediction_column_name: 'system_solution'})\\\n",
                "        .apply(pd.Series.to_dict, axis='columns')\\\n",
                "        .apply(TextTestCase.from_json)\n",
                "    eval_result = [\n",
                "        evaluator.evaluate_test_case(\n",
                "            rating_suite, \n",
                "            test_case, \n",
                "            logger.bind(test_case_id=index)\n",
                "        ) for index, test_case in enumerate(dataset_in_flock_format)\n",
                "    ]\n",
                "    metrics = [r.metrics for r in eval_result]\n",
                "    return pd.DataFrame.from_records(metrics)\\\n",
                "        .add_prefix(f'{prediction_column_name}_', axis='columns')\n",
                "    \n",
                "flock_claude_result_df = evaluate_with_flock(df, 'claude_summary')\n",
                "flock_jurassic_result_df = evaluate_with_flock(df, 'jurassic_summary')\n",
                "flock_titan_result_df = evaluate_with_flock(df, 'titan_summary')\n",
                "df = pd.concat([df, flock_claude_result_df, flock_jurassic_result_df, flock_titan_result_df], axis='columns')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results - Cosine similarity evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            # \"article\", \"claude_summary\", \"jurassic_summary\",\n",
                "            \"claude_cosine_similarity\", \"jurassic_cosine_similarity\", \"titan_cosine_similarity\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_cosine_similarity\", \"jurassic_cosine_similarity\", \"titan_cosine_similarity\"]].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results - RougeL evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            # \"article\", \"claude_summary\", \"jurassic_summary\",\n",
                "            \"claude_rouge_l\", \"jurassic_rouge_l\", \"titan_rouge_l\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_rouge_l\", \"jurassic_rouge_l\", \"titan_rouge_l\"]].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results - LLM-as-a-judge - accuracy evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            # \"article\", \"claude_summary\", \"jurassic_summary\",\n",
                "            \"claude_accuracy\", \"jurassic_accuracy\", \"titan_accuracy\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_accuracy\", \"jurassic_accuracy\", \"titan_accuracy\"]].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results - LLM-as-a-judge - coherence evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            # \"article\", \"claude_summary\", \"jurassic_summary\",\n",
                "            \"claude_coherence\", \"jurassic_coherence\", \"titan_coherence\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_coherence\", \"jurassic_coherence\", \"titan_coherence\"]].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results - LLM-as-a-judge - factuality evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            # \"article\", \"claude_summary\", \"jurassic_summary\",\n",
                "            \"claude_factuality\", \"jurassic_factuality\", \"titan_factuality\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_factuality\", \"jurassic_factuality\", \"titan_factuality\"]].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results - LLM-as-a-judge - completeness evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            # \"article\", \"claude_summary\", \"jurassic_summary\",\n",
                "            \"claude_completeness\", \"jurassic_completeness\", \"titan_completeness\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "    )\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_completeness\", \"jurassic_completeness\", \"titan_completeness\"]].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Flock evaluation results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Flock - semantic similarity evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            \"claude_summary_semantic\", \"jurassic_summary_semantic\", \"titan_summary_semantic\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_summary_semantic\", \"jurassic_summary_semantic\", \"titan_summary_semantic\"]].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Flock - RougeL-Sum evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            \"claude_summary_rouge\", \"jurassic_summary_rouge\", \"titan_summary_rouge\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_summary_rouge\", \"jurassic_summary_rouge\", \"titan_summary_rouge\"]].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Flock - LLM-as-a-judge correctness evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            \"claude_summary_llm_correctness\", \"jurassic_summary_llm_correctness\", \"titan_summary_llm_correctness\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_summary_llm_correctness\", \"jurassic_summary_llm_correctness\", \"titan_summary_llm_correctness\"]].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Flock - LLM-as-a-judge similarity evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            \"claude_summary_llm\", \"jurassic_summary_llm\", \"titan_summary_llm\"\n",
                "        ]].to_html(justify=\"left\")\n",
                "        .replace(\"\\\\n\", \"\")\n",
                "        .replace(\"<td>\", '<td style=\"text-align:left\">')\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[[\"claude_summary_llm\", \"jurassic_summary_llm\", \"titan_summary_llm\"]].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(\n",
                "    HTML(\n",
                "        df[[\n",
                "            \"article\", \"claude_summary\", \"jurassic_summary\", \"titan_summary\",\n",
                "        ]].to_html(justify=\"left\")\n",
                "    )\n",
                ")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
